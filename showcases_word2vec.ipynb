{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| entity | semantic |\n",
    "|-|-|\n",
    "| `data_` | dataframes containing both raw data and targets |\n",
    "| `i_` | indices in `data_` objects |\n",
    "| `y_` | targets |\n",
    "| `f_` | features |\n",
    "| `p_` | predictions of models (incl. cross-validated) |\n",
    "| `q_` | quality metrics (log loss) |\n",
    "| `t_` | transformers |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 412 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 78.3 ms\n"
     ]
    }
   ],
   "source": [
    "data_train = pandas.read_csv('./input/train.csv')\n",
    "data_test = pandas.read_csv('./input/test.csv')\n",
    "data = pandas.concat([data_train, data_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 77.7 ms\n"
     ]
    }
   ],
   "source": [
    "y = data.author\n",
    "\n",
    "i_train = ~y.isnull()\n",
    "i_test = y.isnull()\n",
    "\n",
    "y_train = y[i_train]\n",
    "y_test = y[i_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 91.1 ms\n"
     ]
    }
   ],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 84.7 ms\n"
     ]
    }
   ],
   "source": [
    "def words(s):\n",
    "    return re.findall(r'\\w+', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 328 ms\n"
     ]
    }
   ],
   "source": [
    "f_n_words = (\n",
    "    data[\"text\"]\n",
    "    .apply(lambda s: len(words(s)))\n",
    "    .as_matrix()\n",
    "    [:, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 352 ms\n"
     ]
    }
   ],
   "source": [
    "f_n_unique_words = (\n",
    "    data[\"text\"]\n",
    "    .apply(lambda s: len(set(words(s.lower()))))\n",
    "    .as_matrix()\n",
    "    [:, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.8 ms\n"
     ]
    }
   ],
   "source": [
    "f_n_chars = (\n",
    "    data[\"text\"]\n",
    "    .apply(lambda s: len(s))\n",
    "    .as_matrix()\n",
    "    [:, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 742 ms\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.92 ms\n"
     ]
    }
   ],
   "source": [
    "english_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 419 ms\n"
     ]
    }
   ],
   "source": [
    "f_n_stopwords = (\n",
    "    data[\"text\"]\n",
    "    .apply(lambda s: len([\n",
    "        w\n",
    "        for w in words(s.lower())\n",
    "        if w in english_stopwords\n",
    "    ]))\n",
    "    .as_matrix()\n",
    "    [:, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of punctuation characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 740 µs\n"
     ]
    }
   ],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 437 ms\n"
     ]
    }
   ],
   "source": [
    "f_n_punct = (\n",
    "    data[\"text\"]\n",
    "    .apply(lambda s: len([\n",
    "        c\n",
    "        for c in s\n",
    "        if c in string.punctuation\n",
    "    ]))\n",
    "    .as_matrix()\n",
    "    [:, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of upper-case words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 340 ms\n"
     ]
    }
   ],
   "source": [
    "f_n_upper_words = (\n",
    "    data[\"text\"]\n",
    "    .apply(lambda s: len([\n",
    "        w\n",
    "        for w in words(s)\n",
    "        if w.isupper()\n",
    "    ]))\n",
    "    .as_matrix()\n",
    "    [:, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of title-case words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 350 ms\n"
     ]
    }
   ],
   "source": [
    "f_n_title_words = (\n",
    "    data[\"text\"]\n",
    "    .apply(lambda s: len([\n",
    "        w\n",
    "        for w in words(s)\n",
    "        if w.istitle()\n",
    "    ]))\n",
    "    .as_matrix()\n",
    "    [:, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean length of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 706 µs\n"
     ]
    }
   ],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "f_mean_word_length = (\n",
    "    data[\"text\"]\n",
    "    .apply(lambda s: numpy.mean([\n",
    "        len(w)\n",
    "        for w in words(s)\n",
    "    ]))\n",
    "    .as_matrix()\n",
    "    [:, None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.04 ms\n"
     ]
    }
   ],
   "source": [
    "basic_features = (\n",
    "    f_n_words,\n",
    "    f_n_unique_words,\n",
    "    f_n_chars,\n",
    "    f_n_stopwords,\n",
    "    f_n_punct,\n",
    "    f_n_upper_words,\n",
    "    f_n_title_words,\n",
    "    f_mean_word_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 101 ms\n"
     ]
    }
   ],
   "source": [
    "f_basic_features = numpy.hstack(basic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 162 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 85.3 ms\n"
     ]
    }
   ],
   "source": [
    "t_tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "f_tfidf = t_tfidf.fit_transform(data[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.07 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "n_components = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "t_svd = TruncatedSVD(n_components=n_components, algorithm=\"arpack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "f_svd = t_svd.fit_transform(f_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 980 µs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "t_count = CountVectorizer(stop_words=\"english\", ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.97 s\n"
     ]
    }
   ],
   "source": [
    "f_counts = t_count.fit_transform(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.2 s\n"
     ]
    }
   ],
   "source": [
    "t_svdc = TruncatedSVD(n_components=n_components, algorithm=\"arpack\")\n",
    "f_svdc = t_svdc.fit_transform(f_counts.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.05 ms\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 121 ms\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 93.8 ms\n"
     ]
    }
   ],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 83.6 ms\n"
     ]
    }
   ],
   "source": [
    "def tokenize_s(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27 s\n"
     ]
    }
   ],
   "source": [
    "t_tfidf_s = TfidfVectorizer(\n",
    "    tokenizer=tokenize_s,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    ")\n",
    "f_tfidf_s = t_tfidf_s.fit_transform(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.27 s\n"
     ]
    }
   ],
   "source": [
    "t_svd_s = TruncatedSVD(n_components=n_components, algorithm=\"arpack\")\n",
    "f_svd_s = t_svd_s.fit_transform(f_tfidf_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "t_count_s = CountVectorizer(\n",
    "    tokenizer=tokenize_s,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    ")\n",
    "f_counts_s = t_count_s.fit_transform(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.73 s\n"
     ]
    }
   ],
   "source": [
    "t_svdc_s = TruncatedSVD(n_components=n_components, algorithm=\"arpack\")\n",
    "f_svdc_s = t_svdc_s.fit_transform(f_counts_s.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.12 ms\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "def tokenize_l(text):\n",
    "    return [\n",
    "        lemmatizer.lemmatize(i, j[0].lower())\n",
    "        if j[0].lower() in ['a', 'n', 'v']\n",
    "        else lemmatizer.lemmatize(i)\n",
    "        for i, j in pos_tag(\n",
    "            word_tokenize(text.lower())\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "t_tfidf_l = TfidfVectorizer(\n",
    "    tokenizer=tokenize_l,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    ")\n",
    "f_tfidf_l = t_tfidf_l.fit_transform(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "t_svd_l = TruncatedSVD(n_components=n_components, algorithm=\"arpack\")\n",
    "f_svd_l = t_svd_l.fit_transform(f_tfidf_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "t_count_l = CountVectorizer(\n",
    "    tokenizer=tokenize_l,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),\n",
    ")\n",
    "f_counts_l = t_count_l.fit_transform(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.28 s\n"
     ]
    }
   ],
   "source": [
    "t_svdc_l = TruncatedSVD(n_components=n_components, algorithm=\"arpack\")\n",
    "f_svdc_l = t_svdc_l.fit_transform(f_counts_l.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Character-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "t_tfidf_c = TfidfVectorizer(analyzer=\"char\", ngram_range=(1, 7))\n",
    "f_tfidf_c = t_tfidf_c.fit_transform(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "t_svd_c = TruncatedSVD(n_components=n_components, algorithm=\"arpack\")\n",
    "f_svd_c = t_svd_c.fit_transform(f_tfidf_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "t_count_c = CountVectorizer(analyzer=\"char\", ngram_range=(1, 7))\n",
    "f_counts_c = t_count_c.fit_transform(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "t_svdc_c = TruncatedSVD(n_components=n_components, algorithm=\"arpack\")\n",
    "f_svdc_c = t_svdc_c.fit_transform(f_counts_c.astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.03 ms\n"
     ]
    }
   ],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16G\n",
      "-rw-rw-r--  1 mityajj mityajj 5.3G Oct 24  2015 glove.840B.300d.txt\n",
      "-rw-rw-r--  1 mityajj mityajj 4.7G Oct 25  2015 glove.42B.300d.txt\n",
      "-rw-rw-r--  1 mityajj mityajj 2.0G Aug 14  2014 glove.twitter.27B.200d.txt\n",
      "-rw-rw-r--  1 mityajj mityajj 990M Aug 27  2014 glove.6B.300d.txt\n",
      "-rw-rw-r--  1 mityajj mityajj 975M Aug 14  2014 glove.twitter.27B.100d.txt\n",
      "-rw-rw-r--  1 mityajj mityajj 662M Aug  5  2014 glove.6B.200d.txt\n",
      "-rw-rw-r--  1 mityajj mityajj 488M Aug 14  2014 glove.twitter.27B.50d.txt\n",
      "-rw-rw-r--  1 mityajj mityajj 332M Aug  5  2014 glove.6B.100d.txt\n",
      "-r--r--r--  1 mityajj mityajj 246M Aug 14  2014 glove.twitter.27B.25d.txt\n",
      "-rw-rw-r--  1 mityajj mityajj 164M Aug  5  2014 glove.6B.50d.txt\n",
      "drwxr-xr-x  3 mityajj mityajj 4.0K Dec  4 23:51 .\n",
      "drwx------ 42 mityajj mityajj 4.0K Dec  4 23:04 ..\n",
      "drwxr-xr-x  2 mityajj mityajj 4.0K Dec  4 23:51 packed\n",
      "time: 278 ms\n"
     ]
    }
   ],
   "source": [
    "!ls -alhS /home/mityajj/nlp_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1917494 /home/mityajj/nlp_models/glove.42B.300d.txt\n",
      "     400000 /home/mityajj/nlp_models/glove.6B.100d.txt\n",
      "     400000 /home/mityajj/nlp_models/glove.6B.200d.txt\n",
      "     400000 /home/mityajj/nlp_models/glove.6B.300d.txt\n",
      "     400000 /home/mityajj/nlp_models/glove.6B.50d.txt\n",
      "    2196017 /home/mityajj/nlp_models/glove.840B.300d.txt\n",
      "    1193514 /home/mityajj/nlp_models/glove.twitter.27B.100d.txt\n",
      "    1193514 /home/mityajj/nlp_models/glove.twitter.27B.200d.txt\n",
      "    1193514 /home/mityajj/nlp_models/glove.twitter.27B.25d.txt\n",
      "    1193514 /home/mityajj/nlp_models/glove.twitter.27B.50d.txt\n",
      "   10487567 total\n",
      "time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "!wc -l /home/mityajj/nlp_models/*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.77 ms\n"
     ]
    }
   ],
   "source": [
    "fn_glove_6B_50d = \"/home/mityajj/nlp_models/glove.6B.50d.txt\"  # 400k\n",
    "fn_glove_6B_100d = \"/home/mityajj/nlp_models/glove.6B.100d.txt\"  # 400k\n",
    "fn_glove_27B_50d = \"/home/mityajj/nlp_models/glove.twitter.27B.50d.txt\"  # 1193514\n",
    "fn_glove_42B_50d = \"/home/mityajj/nlp_models/glove.42B.300d.txt\" # 1917494\n",
    "fn_glove_840B_300d = \"/home/mityajj/nlp_models/glove.840B.300d.txt\"  # 2196017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "def load_w2v(file_name, total=None):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        return {\n",
    "            line.split()[0]: numpy.array([float(x) for x in line.split()[1:]])\n",
    "            for line in tqdm.tqdm(f, total=total)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:09<00:00, 43065.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.51 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_6B_50d = load_w2v(fn_glove_6B_50d, total=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:18<00:00, 21710.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_6B_100d = load_w2v(fn_glove_6B_100d, total=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.47 ms\n"
     ]
    }
   ],
   "source": [
    "def w2v_mean(w2v):\n",
    "    def w2v_mean_impl(t):\n",
    "        vectors = [\n",
    "            w2v[w]\n",
    "            for w in word_tokenize(t.lower())\n",
    "            if w in w2v\n",
    "        ]\n",
    "        if vectors:\n",
    "            return numpy.mean(vectors, axis=0)\n",
    "        else:\n",
    "            return numpy.zeros_like(next(iter(w2v.items()))[1])\n",
    "    return w2v_mean_impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 105 ms\n"
     ]
    }
   ],
   "source": [
    "w2v_6B_50d_mean = w2v_mean(w2v_6B_50d)\n",
    "w2v_6B_100d_mean = w2v_mean(w2v_6B_100d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.85 s\n"
     ]
    }
   ],
   "source": [
    "f_w2v_6B_50d_mean = numpy.vstack(data.text.apply(w2v_6B_50d_mean).as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.63 s\n"
     ]
    }
   ],
   "source": [
    "f_w2v_6B_100d_mean = numpy.vstack(data.text.apply(w2v_6B_100d_mean).as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 794 µs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 95.7 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 84.5 ms\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 74.4 ms\n"
     ]
    }
   ],
   "source": [
    "def apply_model(\n",
    "    model,\n",
    "    features,\n",
    "    evaluate=True,\n",
    "    predict=False,\n",
    "):\n",
    "    if any(map(\n",
    "        lambda z: type(z) in [\n",
    "            scipy.sparse.csr_matrix,\n",
    "            scipy.sparse.csc_matrix,\n",
    "            scipy.sparse.coo_matrix,\n",
    "        ],\n",
    "        features,\n",
    "    )):\n",
    "        hstack = scipy.sparse.hstack\n",
    "    else:\n",
    "        hstack = numpy.hstack\n",
    "    \n",
    "    f_all = hstack(features)\n",
    "    f_train = f_all[numpy.nonzero(i_train)]\n",
    "    f_test = f_all[numpy.nonzero(i_test)]\n",
    "    \n",
    "    p_cv = cross_val_predict(\n",
    "        model,\n",
    "        f_train,\n",
    "        y_train,\n",
    "        cv=cv,\n",
    "        method=\"predict_proba\",\n",
    "    )\n",
    "    q_cv = log_loss(y_train, p_cv)\n",
    "    \n",
    "    model.fit(f_train, y_train)\n",
    "    \n",
    "    p_train = model.predict_proba(f_train)\n",
    "    q_train = log_loss(y_train, p_train)\n",
    "    \n",
    "    if evaluate:\n",
    "        print(f\"train log loss = {q_train:.5f}\")\n",
    "        print(f\"   cv log loss = {q_cv:.5f}\")\n",
    "        print()\n",
    "    \n",
    "    if predict:\n",
    "        p_test = model.predict_proba(f_test)\n",
    "        p_full = numpy.concatenate((p_cv, p_test), axis=0)\n",
    "        return pandas.DataFrame(p_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 129 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 1.00201\n",
      "   cv log loss = 1.00285\n",
      "\n",
      "time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_basic = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    basic_features,\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.83138\n",
      "   cv log loss = 0.83611\n",
      "\n",
      "time: 6.08 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_w2v_6B_50d_mean = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_w2v_6B_50d_mean, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.77558\n",
      "   cv log loss = 0.78560\n",
      "\n",
      "time: 9.04 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_w2v_6B_100d_mean = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_w2v_6B_100d_mean, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.60284\n",
      "   cv log loss = 0.83517\n",
      "\n",
      "time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_tfidf = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_tfidf, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.97553\n",
      "   cv log loss = 0.98460\n",
      "\n",
      "time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_svd = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_svd, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.11811\n",
      "   cv log loss = 0.53677\n",
      "\n",
      "time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_counts = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_counts, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.98263\n",
      "   cv log loss = 0.98512\n",
      "\n",
      "time: 2.43 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_svdc = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_svdc, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.47451\n",
      "   cv log loss = 0.62743\n",
      "\n",
      "time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "p_lr_tfidf_c = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_tfidf_c, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.86968\n",
      "   cv log loss = 0.87547\n",
      "\n",
      "time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_svd_c = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_svd_c, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.27036\n",
      "   cv log loss = 0.46097\n",
      "\n",
      "time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "p_lr_counts_c = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_counts_c, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.93961\n",
      "   cv log loss = 0.94175\n",
      "\n",
      "time: 2.93 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_svdc_c = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_svdc_c, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK stems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.56389\n",
      "   cv log loss = 0.77756\n",
      "\n",
      "time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_tfidf_s = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_tfidf_s, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.89632\n",
      "   cv log loss = 0.90756\n",
      "\n",
      "time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_svd_s = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_svd_s, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.08449\n",
      "   cv log loss = 0.49929\n",
      "\n",
      "time: 47.4 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_counts_s = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_counts_s, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.94342\n",
      "   cv log loss = 0.94601\n",
      "\n",
      "time: 4.92 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_svdc_s = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_svdc_s, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.57171\n",
      "   cv log loss = 0.78810\n",
      "\n",
      "time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_tfidf_l = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_tfidf_l, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.91920\n",
      "   cv log loss = 0.92882\n",
      "\n",
      "time: 2.17 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_svd_l = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_svd_l, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.09439\n",
      "   cv log loss = 0.51219\n",
      "\n",
      "time: 37 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_counts_l = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_counts_l, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.91874\n",
      "   cv log loss = 0.92122\n",
      "\n",
      "time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "p_lr_svdc_l = apply_model(\n",
    "    LogisticRegression(max_iter=10),\n",
    "    (f_svdc_l, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.18 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apply_model(\n",
    "    CalibratedClassifierCV(\n",
    "        LogisticRegression(max_iter=10),\n",
    "        method=\"sigmoid\",\n",
    "    ),\n",
    "    (f_counts_c, ),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train log loss = 0.26277\n",
    "   cv log loss = 0.46686"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "apply_model(\n",
    "    CalibratedClassifierCV(\n",
    "        LogisticRegression(max_iter=10),\n",
    "        method=\"isotonic\",\n",
    "    ),\n",
    "    (f_counts_c, ),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train log loss = 0.25385\n",
    "   cv log loss = 0.46363"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 112 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.46620\n",
      "   cv log loss = 0.84280\n",
      "\n",
      "time: 826 ms\n"
     ]
    }
   ],
   "source": [
    "p_nb_tfidf = apply_model(\n",
    "    MultinomialNB(),\n",
    "    (f_tfidf, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** SVD - does not work for NB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.03011\n",
      "   cv log loss = 0.45322\n",
      "\n",
      "time: 711 ms\n"
     ]
    }
   ],
   "source": [
    "p_nb_counts = apply_model(\n",
    "    MultinomialNB(),\n",
    "    (f_counts, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.65026\n",
      "   cv log loss = 0.91851\n",
      "\n",
      "time: 6.75 s\n"
     ]
    }
   ],
   "source": [
    "p_nb_tfidf_c = apply_model(\n",
    "    MultinomialNB(),\n",
    "    (f_tfidf_c, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.96735\n",
      "   cv log loss = 3.77831\n",
      "\n",
      "time: 7.18 s\n"
     ]
    }
   ],
   "source": [
    "p_nb_counts_c = apply_model(\n",
    "    MultinomialNB(),\n",
    "    (f_counts_c, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK stems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.44457\n",
      "   cv log loss = 0.80402\n",
      "\n",
      "time: 812 ms\n"
     ]
    }
   ],
   "source": [
    "p_nb_tfidf_s = apply_model(\n",
    "    MultinomialNB(),\n",
    "    (f_tfidf_s, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.02554\n",
      "   cv log loss = 0.64781\n",
      "\n",
      "time: 834 ms\n"
     ]
    }
   ],
   "source": [
    "p_nb_counts_s = apply_model(\n",
    "    MultinomialNB(),\n",
    "    (f_counts_s, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.44638\n",
      "   cv log loss = 0.80741\n",
      "\n",
      "time: 776 ms\n"
     ]
    }
   ],
   "source": [
    "p_nb_tfidf_l = apply_model(\n",
    "    MultinomialNB(),\n",
    "    (f_tfidf_l, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.03101\n",
      "   cv log loss = 0.62511\n",
      "\n",
      "time: 786 ms\n"
     ]
    }
   ],
   "source": [
    "p_nb_counts_l = apply_model(\n",
    "    MultinomialNB(),\n",
    "    (f_counts_l, ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.4 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mityajj/kaggle/venv36.ds/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.96825\n",
      "   cv log loss = 0.99003\n",
      "\n",
      "time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "apply_model(\n",
    "    XGBClassifier(n_estimators=100),\n",
    "    basic_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.79868\n",
      "   cv log loss = 0.83843\n",
      "\n",
      "time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "apply_model(\n",
    "    XGBClassifier(n_estimators=100),\n",
    "    (f_svd, ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.79076\n",
      "   cv log loss = 0.84265\n",
      "\n",
      "time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "apply_model(\n",
    "    XGBClassifier(n_estimators=100),\n",
    "    (f_w2v_6B_50d_mean, ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.74976\n",
      "   cv log loss = 0.80907\n",
      "\n",
      "time: 46.3 s\n"
     ]
    }
   ],
   "source": [
    "apply_model(\n",
    "    XGBClassifier(n_estimators=100),\n",
    "    (f_w2v_6B_100d_mean, ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.26724\n",
      "   cv log loss = 0.30961\n",
      "\n",
      "time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "p_xgb_6 = apply_model(\n",
    "    XGBClassifier(n_estimators=100),\n",
    "    basic_features + (\n",
    "        f_svd,\n",
    "#         f_svd_c,\n",
    "        f_svd_s,\n",
    "        f_svd_l,\n",
    "        \n",
    "        # none - 31093\n",
    "        # 1st - 31082\n",
    "        # 2nd - 31065\n",
    "        # all - 30999\n",
    "        \n",
    "        # SVDs\n",
    "#         f_svdc,  # 1) 30981\n",
    "        f_svdc_c,  # 2) 31009\n",
    "        f_svdc_l,  # 3) 31048\n",
    "        f_svdc_s,  # 4) 30984\n",
    "        \n",
    "        # SVD-trained\n",
    "        p_lr_svdc,  # 5) 31091\n",
    "        p_lr_svdc_c,  # 6) 31009\n",
    "        p_lr_svdc_l,  # 7) 30997\n",
    "        p_lr_svdc_s,  # 8) 30981\n",
    "        \n",
    "        p_lr_basic,\n",
    "        \n",
    "        p_lr_tfidf,\n",
    "        p_lr_svd,\n",
    "#         p_lr_counts,\n",
    "        \n",
    "        p_lr_tfidf_c,\n",
    "        p_lr_svd_c,\n",
    "        p_lr_counts_c,\n",
    "        \n",
    "        p_lr_tfidf_s,\n",
    "#         p_lr_svd_s,\n",
    "        p_lr_counts_s,\n",
    "        \n",
    "#         p_lr_tfidf_l,\n",
    "        p_lr_svd_l,\n",
    "#         p_lr_counts_l,\n",
    "        \n",
    "#         p_nb_tfidf,\n",
    "        p_nb_counts,\n",
    "        \n",
    "        # these two features removed from best model\n",
    "        p_nb_tfidf_c,\n",
    "        p_nb_counts_c,\n",
    "        \n",
    "        p_nb_tfidf_s,\n",
    "        p_nb_counts_s,\n",
    "        \n",
    "#         p_nb_tfidf_l,\n",
    "#         p_nb_counts_l,\n",
    "    ),\n",
    "#     predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.26585\n",
      "   cv log loss = 0.30774\n",
      "\n",
      "time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "p_xgb_6 = apply_model(\n",
    "    XGBClassifier(n_estimators=100),\n",
    "    basic_features + (\n",
    "        f_w2v_6B_50d_mean,\n",
    "        \n",
    "        f_svd,\n",
    "#         f_svd_c,\n",
    "        f_svd_s,\n",
    "        f_svd_l,\n",
    "        \n",
    "        # none - 31093\n",
    "        # 1st - 31082\n",
    "        # 2nd - 31065\n",
    "        # all - 30999\n",
    "        \n",
    "        # SVDs\n",
    "#         f_svdc,  # 1) 30981\n",
    "        f_svdc_c,  # 2) 31009\n",
    "        f_svdc_l,  # 3) 31048\n",
    "        f_svdc_s,  # 4) 30984\n",
    "        \n",
    "        # SVD-trained\n",
    "        p_lr_svdc,  # 5) 31091\n",
    "        p_lr_svdc_c,  # 6) 31009\n",
    "        p_lr_svdc_l,  # 7) 30997\n",
    "        p_lr_svdc_s,  # 8) 30981\n",
    "        \n",
    "        p_lr_basic,\n",
    "        \n",
    "        p_lr_tfidf,\n",
    "        p_lr_svd,\n",
    "#         p_lr_counts,\n",
    "        \n",
    "        p_lr_tfidf_c,\n",
    "        p_lr_svd_c,\n",
    "        p_lr_counts_c,\n",
    "        \n",
    "        p_lr_tfidf_s,\n",
    "#         p_lr_svd_s,\n",
    "        p_lr_counts_s,\n",
    "        \n",
    "#         p_lr_tfidf_l,\n",
    "        p_lr_svd_l,\n",
    "#         p_lr_counts_l,\n",
    "        \n",
    "#         p_nb_tfidf,\n",
    "        p_nb_counts,\n",
    "        \n",
    "        # these two features removed from best model\n",
    "        p_nb_tfidf_c,\n",
    "        p_nb_counts_c,\n",
    "        \n",
    "        p_nb_tfidf_s,\n",
    "        p_nb_counts_s,\n",
    "        \n",
    "#         p_nb_tfidf_l,\n",
    "#         p_nb_counts_l,\n",
    "    ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.26429\n",
      "   cv log loss = 0.30779\n",
      "\n",
      "time: 3min 54s\n"
     ]
    }
   ],
   "source": [
    "p_xgb_6 = apply_model(\n",
    "    XGBClassifier(n_estimators=100),\n",
    "    basic_features + (\n",
    "        f_w2v_6B_100d_mean,\n",
    "        \n",
    "        f_svd,\n",
    "#         f_svd_c,\n",
    "        f_svd_s,\n",
    "        f_svd_l,\n",
    "        \n",
    "        # none - 31093\n",
    "        # 1st - 31082\n",
    "        # 2nd - 31065\n",
    "        # all - 30999\n",
    "        \n",
    "        # SVDs\n",
    "#         f_svdc,  # 1) 30981\n",
    "        f_svdc_c,  # 2) 31009\n",
    "        f_svdc_l,  # 3) 31048\n",
    "        f_svdc_s,  # 4) 30984\n",
    "        \n",
    "        # SVD-trained\n",
    "        p_lr_svdc,  # 5) 31091\n",
    "        p_lr_svdc_c,  # 6) 31009\n",
    "        p_lr_svdc_l,  # 7) 30997\n",
    "        p_lr_svdc_s,  # 8) 30981\n",
    "        \n",
    "        p_lr_basic,\n",
    "        \n",
    "        p_lr_tfidf,\n",
    "        p_lr_svd,\n",
    "#         p_lr_counts,\n",
    "        \n",
    "        p_lr_tfidf_c,\n",
    "        p_lr_svd_c,\n",
    "        p_lr_counts_c,\n",
    "        \n",
    "        p_lr_tfidf_s,\n",
    "#         p_lr_svd_s,\n",
    "        p_lr_counts_s,\n",
    "        \n",
    "#         p_lr_tfidf_l,\n",
    "        p_lr_svd_l,\n",
    "#         p_lr_counts_l,\n",
    "        \n",
    "#         p_nb_tfidf,\n",
    "        p_nb_counts,\n",
    "        \n",
    "        # these two features removed from best model\n",
    "        p_nb_tfidf_c,\n",
    "        p_nb_counts_c,\n",
    "        \n",
    "        p_nb_tfidf_s,\n",
    "        p_nb_counts_s,\n",
    "        \n",
    "#         p_nb_tfidf_l,\n",
    "#         p_nb_counts_l,\n",
    "    ),\n",
    "#     predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibrated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ci_xgb_4 = apply_model(\n",
    "    CalibratedClassifierCV(\n",
    "        XGBClassifier(n_estimators=100),\n",
    "        method=\"isotonic\",\n",
    "    ),\n",
    "    basic_features + (\n",
    "        f_svd,\n",
    "#         f_svd_c,\n",
    "        f_svd_s,\n",
    "        f_svd_l,\n",
    "        \n",
    "        # none - 31093\n",
    "        # 1st - 31082\n",
    "        # 2nd - 31065\n",
    "        # all - 30999\n",
    "        \n",
    "        # SVDs\n",
    "#         f_svdc,  # 1) 30981\n",
    "        f_svdc_c,  # 2) 31009\n",
    "        f_svdc_l,  # 3) 31048\n",
    "        f_svdc_s,  # 4) 30984\n",
    "        \n",
    "        # SVD-trained\n",
    "        p_lr_svdc,  # 5) 31091\n",
    "        p_lr_svdc_c,  # 6) 31009\n",
    "        p_lr_svdc_l,  # 7) 30997\n",
    "        p_lr_svdc_s,  # 8) 30981\n",
    "        \n",
    "        p_lr_basic,\n",
    "        \n",
    "        p_lr_tfidf,\n",
    "        p_lr_svd,\n",
    "#         p_lr_counts,\n",
    "        \n",
    "        p_lr_tfidf_c,\n",
    "        p_lr_svd_c,\n",
    "        p_lr_counts_c,\n",
    "        \n",
    "        p_lr_tfidf_s,\n",
    "#         p_lr_svd_s,\n",
    "        p_lr_counts_s,\n",
    "        \n",
    "#         p_lr_tfidf_l,\n",
    "        p_lr_svd_l,\n",
    "#         p_lr_counts_l,\n",
    "        \n",
    "#         p_nb_tfidf,\n",
    "        p_nb_counts,\n",
    "        \n",
    "        # these two features removed from best model\n",
    "        p_nb_tfidf_c,\n",
    "        p_nb_counts_c,\n",
    "        \n",
    "        p_nb_tfidf_s,\n",
    "        p_nb_counts_s,\n",
    "        \n",
    "#         p_nb_tfidf_l,\n",
    "#         p_nb_counts_l,\n",
    "    ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train log loss = 0.25934\n",
      "   cv log loss = 0.30740\n",
      "\n",
      "time: 9min 4s\n"
     ]
    }
   ],
   "source": [
    "p_ci_xgb_6 = apply_model(\n",
    "    CalibratedClassifierCV(\n",
    "        XGBClassifier(n_estimators=100),\n",
    "        method=\"isotonic\",\n",
    "    ),\n",
    "    basic_features + (\n",
    "        f_w2v_6B_50d_mean,\n",
    "        \n",
    "        f_svd,\n",
    "#         f_svd_c,\n",
    "        f_svd_s,\n",
    "        f_svd_l,\n",
    "        \n",
    "        # none - 31093\n",
    "        # 1st - 31082\n",
    "        # 2nd - 31065\n",
    "        # all - 30999\n",
    "        \n",
    "        # SVDs\n",
    "#         f_svdc,  # 1) 30981\n",
    "        f_svdc_c,  # 2) 31009\n",
    "        f_svdc_l,  # 3) 31048\n",
    "        f_svdc_s,  # 4) 30984\n",
    "        \n",
    "        # SVD-trained\n",
    "        p_lr_svdc,  # 5) 31091\n",
    "        p_lr_svdc_c,  # 6) 31009\n",
    "        p_lr_svdc_l,  # 7) 30997\n",
    "        p_lr_svdc_s,  # 8) 30981\n",
    "        \n",
    "        p_lr_basic,\n",
    "        \n",
    "        p_lr_tfidf,\n",
    "        p_lr_svd,\n",
    "#         p_lr_counts,\n",
    "        \n",
    "        p_lr_tfidf_c,\n",
    "        p_lr_svd_c,\n",
    "        p_lr_counts_c,\n",
    "        \n",
    "        p_lr_tfidf_s,\n",
    "#         p_lr_svd_s,\n",
    "        p_lr_counts_s,\n",
    "        \n",
    "#         p_lr_tfidf_l,\n",
    "        p_lr_svd_l,\n",
    "#         p_lr_counts_l,\n",
    "        \n",
    "#         p_nb_tfidf,\n",
    "        p_nb_counts,\n",
    "        \n",
    "        # these two features removed from best model\n",
    "        p_nb_tfidf_c,\n",
    "        p_nb_counts_c,\n",
    "        \n",
    "        p_nb_tfidf_s,\n",
    "        p_nb_counts_s,\n",
    "        \n",
    "#         p_nb_tfidf_l,\n",
    "#         p_nb_counts_l,\n",
    "    ),\n",
    "    predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.22 ms\n"
     ]
    }
   ],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 131 ms\n"
     ]
    }
   ],
   "source": [
    "submission_path = os.path.realpath('./output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 95.6 ms\n"
     ]
    }
   ],
   "source": [
    "def make_submission(predictions, file_name):\n",
    "    file_name = os.path.join(\n",
    "        submission_path,\n",
    "        os.path.basename(file_name),\n",
    "    )\n",
    "    predictions = predictions.copy()\n",
    "    predictions.columns = y_train.unique()\n",
    "    pandas.concat(\n",
    "        (\n",
    "            data_test[\"id\"].reset_index(drop=True),\n",
    "            predictions[i_test].reset_index(drop=True),\n",
    "        ),\n",
    "        axis=1,\n",
    "    ).to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 51.8 ms\n"
     ]
    }
   ],
   "source": [
    "make_submission(p_xgb_6, \"p_xgb_6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 143 ms\n"
     ]
    }
   ],
   "source": [
    "make_submission(p_ci_xgb_6, \"p_ci_xgb_6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
